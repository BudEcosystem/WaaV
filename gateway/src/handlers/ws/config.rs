//! WebSocket configuration types and handlers
//!
//! This module contains all configuration-related types for WebSocket connections,
//! including STT, TTS, and LiveKit configurations without API keys.

use serde::{Deserialize, Serialize};
use xxhash_rust::xxh3::xxh3_128;

use crate::{
    core::{
        emotion::{DeliveryStyle, Emotion, EmotionConfig, EmotionIntensity},
        stt::STTConfig,
        tts::{Pronunciation, TTSConfig},
    },
    livekit::LiveKitConfig,
};

/// Default value for audio enabled flag (true)
pub fn default_audio_enabled() -> Option<bool> {
    Some(true)
}

/// Default value for allow_interruption flag (true)
pub fn default_allow_interruption() -> Option<bool> {
    Some(true)
}

/// STT configuration for WebSocket messages (with optional API key)
#[derive(Debug, Deserialize, Serialize, Clone)]
#[cfg_attr(feature = "openapi", derive(utoipa::ToSchema))]
pub struct STTWebSocketConfig {
    /// Provider name (e.g., "deepgram")
    #[cfg_attr(feature = "openapi", schema(example = "deepgram"))]
    pub provider: String,
    /// Language code for transcription (e.g., "en-US", "es-ES")
    #[cfg_attr(feature = "openapi", schema(example = "en-US"))]
    pub language: String,
    /// Sample rate of the audio in Hz
    #[cfg_attr(feature = "openapi", schema(example = 16000))]
    pub sample_rate: u32,
    /// Number of audio channels (1 for mono, 2 for stereo)
    #[cfg_attr(feature = "openapi", schema(example = 1))]
    pub channels: u16,
    /// Enable punctuation in results
    #[cfg_attr(feature = "openapi", schema(example = true))]
    pub punctuation: bool,
    /// Encoding of the audio
    #[cfg_attr(feature = "openapi", schema(example = "linear16"))]
    pub encoding: String,
    /// Model to use for transcription
    #[cfg_attr(feature = "openapi", schema(example = "nova-2"))]
    pub model: String,
    /// Optional API key for this provider (overrides server config)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub api_key: Option<String>,
}

impl STTWebSocketConfig {
    /// Convert WebSocket STT config to full STT config with API key
    ///
    /// # Arguments
    /// * `api_key` - The API key to use for this provider
    ///
    /// # Returns
    /// * `STTConfig` - Full STT configuration
    pub fn to_stt_config(&self, api_key: String) -> STTConfig {
        STTConfig {
            provider: self.provider.clone(),
            api_key,
            language: self.language.clone(),
            sample_rate: self.sample_rate,
            channels: self.channels,
            punctuation: self.punctuation,
            encoding: self.encoding.clone(),
            model: self.model.clone(),
        }
    }
}

/// LiveKit configuration for WebSocket messages
#[derive(Debug, Deserialize, Serialize, Clone)]
#[cfg_attr(feature = "openapi", derive(utoipa::ToSchema))]
pub struct LiveKitWebSocketConfig {
    /// Room name to join or create
    #[cfg_attr(feature = "openapi", schema(example = "conversation-room-123"))]
    pub room_name: String,
    /// Enable recording for this session
    #[serde(default)]
    pub enable_recording: bool,
    // recording_file_key removed; recording path now determined by stream_id + server prefix
    /// WaaV AI participant identity (defaults to "waav-ai")
    #[serde(skip_serializing_if = "Option::is_none")]
    #[cfg_attr(feature = "openapi", schema(example = "waav-ai"))]
    pub waav_participant_identity: Option<String>,
    /// WaaV AI participant display name (defaults to "WaaV AI")
    #[serde(skip_serializing_if = "Option::is_none")]
    #[cfg_attr(feature = "openapi", schema(example = "WaaV AI"))]
    pub waav_participant_name: Option<String>,
    /// List of participant identities to listen to for audio tracks and data messages. (All participants by default)
    ///
    /// **Behavior**:
    /// - If **empty** (default): Audio tracks and data messages from **all participants** will be processed
    /// - If **populated**: Only audio tracks and data messages from participants whose identities
    ///   are in this list will be processed; others will be ignored
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub listen_participants: Vec<String>,
}

impl LiveKitWebSocketConfig {
    /// Convert WebSocket LiveKit config to full LiveKit config with audio parameters
    ///
    /// # Arguments
    /// * `token` - JWT token for LiveKit room access (generated by LiveKitRoomHandler)
    /// * `tts_config` - TTS configuration containing audio parameters
    /// * `livekit_url` - LiveKit server URL
    ///
    /// # Returns
    /// * `LiveKitConfig` - Full LiveKit configuration with audio parameters
    pub fn to_livekit_config(
        &self,
        token: String,
        tts_config: &TTSWebSocketConfig,
        livekit_url: &str,
    ) -> LiveKitConfig {
        LiveKitConfig {
            url: livekit_url.to_string(),
            token,
            room_name: self.room_name.clone(),
            // Use TTS config sample rate, default to 24000 if not specified
            sample_rate: tts_config.sample_rate.unwrap_or(24000),
            // Assume mono audio for TTS (1 channel)
            channels: 1,
            // Enable noise filter by default when compiled with the optional feature
            // Can be disabled via config if lower latency is needed
            enable_noise_filter: cfg!(feature = "noise-filter"),
            // Pass through the participant filter list
            listen_participants: self.listen_participants.clone(),
        }
    }
}

/// TTS configuration for WebSocket messages (with optional API key)
#[derive(Debug, Deserialize, Serialize, Clone)]
#[cfg_attr(feature = "openapi", derive(utoipa::ToSchema))]
pub struct TTSWebSocketConfig {
    /// Provider name (e.g., "deepgram", "hume", "elevenlabs")
    #[cfg_attr(feature = "openapi", schema(example = "deepgram"))]
    pub provider: String,
    /// Voice ID or name to use for synthesis
    #[cfg_attr(feature = "openapi", schema(example = "aura-asteria-en"))]
    pub voice_id: Option<String>,
    /// Speaking rate (0.25 to 4.0, 1.0 is normal)
    #[cfg_attr(feature = "openapi", schema(example = 1.0))]
    pub speaking_rate: Option<f32>,
    /// Audio format preference
    #[cfg_attr(feature = "openapi", schema(example = "linear16"))]
    pub audio_format: Option<String>,
    /// Sample rate preference
    #[cfg_attr(feature = "openapi", schema(example = 24000))]
    pub sample_rate: Option<u32>,
    /// Connection timeout in seconds
    #[cfg_attr(feature = "openapi", schema(example = 30))]
    pub connection_timeout: Option<u64>,
    /// Request timeout in seconds
    #[cfg_attr(feature = "openapi", schema(example = 60))]
    pub request_timeout: Option<u64>,
    /// Model to use for TTS
    #[cfg_attr(feature = "openapi", schema(example = "aura-asteria-en"))]
    pub model: String,
    /// Pronunciation replacements to apply before TTS
    #[serde(default)]
    pub pronunciations: Vec<Pronunciation>,
    /// Optional API key for this provider (overrides server config)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub api_key: Option<String>,

    // =========================================================================
    // Emotion Control (Unified Emotion System)
    // =========================================================================
    /// Emotion to express in speech synthesis.
    ///
    /// Supported by: Hume AI (all), ElevenLabs (core set), Azure (SSML styles).
    /// Providers without emotion support will synthesize speech normally with a warning.
    ///
    /// Examples: "happy", "sad", "angry", "excited", "calm", "sarcastic"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[cfg_attr(feature = "openapi", schema(example = "happy"))]
    pub emotion: Option<Emotion>,

    /// Intensity of the emotion (0.0 to 1.0, or "low"/"medium"/"high").
    ///
    /// - 0.0-0.3: Subtle expression
    /// - 0.4-0.7: Moderate expression (default: 0.6)
    /// - 0.8-1.0: Strong expression
    ///
    /// Alternatively, use named levels: "low" (0.3), "medium" (0.6), "high" (1.0)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[cfg_attr(feature = "openapi", schema(example = 0.8))]
    pub emotion_intensity: Option<EmotionIntensity>,

    /// Delivery style modifier for speech.
    ///
    /// Affects pacing and prosody independently of emotion.
    /// Examples: "whispered", "shouted", "rushed", "measured", "expressive"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[cfg_attr(feature = "openapi", schema(example = "expressive"))]
    pub delivery_style: Option<DeliveryStyle>,

    /// Free-form emotion description (for providers like Hume AI).
    ///
    /// When provided, this takes precedence over the `emotion` field for Hume AI.
    /// Maximum 100 characters.
    ///
    /// Examples: "warm, friendly, inviting", "whispered fearfully", "sarcastic, dry"
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[cfg_attr(feature = "openapi", schema(example = "warm, friendly, inviting"))]
    pub emotion_description: Option<String>,
}

impl TTSWebSocketConfig {
    /// Convert WebSocket TTS config to full TTS config with API key and proper defaults
    ///
    /// # Arguments
    /// * `api_key` - The API key to use for this provider
    ///
    /// # Returns
    /// * `TTSConfig` - Full TTS configuration with defaults applied
    pub fn to_tts_config(&self, api_key: String) -> TTSConfig {
        // Start with defaults
        let defaults = TTSConfig::default();

        // Extract emotion config from WebSocket config fields
        let emotion_config = self.to_emotion_config();

        TTSConfig {
            provider: self.provider.clone(),
            api_key,
            model: self.model.clone(),
            // Use provided values or fall back to defaults
            voice_id: self.voice_id.clone().or(defaults.voice_id),
            speaking_rate: self.speaking_rate.or(defaults.speaking_rate),
            audio_format: self.audio_format.clone().or(defaults.audio_format),
            sample_rate: self.sample_rate.or(defaults.sample_rate),
            connection_timeout: self.connection_timeout.or(defaults.connection_timeout),
            request_timeout: self.request_timeout.or(defaults.request_timeout),
            pronunciations: self.pronunciations.clone(),
            request_pool_size: defaults.request_pool_size,
            emotion_config,
        }
    }

    /// Extract emotion configuration from WebSocket config.
    ///
    /// Combines all emotion-related fields into a unified `EmotionConfig`.
    /// Returns `None` if no emotion settings are specified.
    ///
    /// # Returns
    ///
    /// An `EmotionConfig` if any emotion fields are set, `None` otherwise.
    ///
    /// # Example
    ///
    /// ```rust,ignore
    /// let ws_config = TTSWebSocketConfig {
    ///     provider: "hume".to_string(),
    ///     emotion: Some(Emotion::Happy),
    ///     emotion_intensity: Some(EmotionIntensity::from_f32(0.8)),
    ///     ..Default::default()
    /// };
    ///
    /// if let Some(emotion_config) = ws_config.to_emotion_config() {
    ///     let mapper = get_mapper_for_provider(&ws_config.provider);
    ///     let mapped = mapper.map_emotion(&emotion_config);
    /// }
    /// ```
    pub fn to_emotion_config(&self) -> Option<EmotionConfig> {
        // Only return config if at least one emotion field is set
        if self.emotion.is_none()
            && self.emotion_intensity.is_none()
            && self.delivery_style.is_none()
            && self.emotion_description.is_none()
        {
            return None;
        }

        Some(EmotionConfig {
            emotion: self.emotion,
            intensity: self.emotion_intensity.clone(),
            style: self.delivery_style,
            description: self.emotion_description.clone(),
            context: None, // Context is not exposed in WebSocket config for simplicity
        })
    }

    /// Returns whether any emotion settings are configured.
    #[inline]
    pub fn has_emotion_config(&self) -> bool {
        self.emotion.is_some()
            || self.emotion_intensity.is_some()
            || self.delivery_style.is_some()
            || self.emotion_description.is_some()
    }
}

/// Compute TTS configuration hash for caching
pub fn compute_tts_config_hash(tts_config: &TTSConfig) -> String {
    let mut s = String::new();
    s.push_str(tts_config.provider.as_str());
    s.push('|');
    s.push_str(tts_config.voice_id.as_deref().unwrap_or(""));
    s.push('|');
    s.push_str(&tts_config.model);
    s.push('|');
    s.push_str(tts_config.audio_format.as_deref().unwrap_or(""));
    s.push('|');
    if let Some(sr) = tts_config.sample_rate {
        s.push_str(&sr.to_string());
    }
    s.push('|');
    if let Some(rate) = tts_config.speaking_rate {
        s.push_str(&format!("{rate:.3}"));
    }
    format!("{:032x}", xxh3_128(s.as_bytes()))
}
